{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The API: Connecting to BigML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The BigML API offers an endpoint to create, get, update and delete every Machine Learning resource. <br/>\n",
    "This API is accessible via HTTP and its general public domain is [bigml.io](https://bigml.io). <br/>\n",
    "You will need some credentials that will be used to authenticate every request. <br/>\n",
    "We recommend to set them in environment variables for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set your credentials as environment variables\n",
    "%env BIGML_USERNAME=merce_demo\n",
    "%env BIGML_API_KEY=***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API calls that you will need to issue contain these credentials as authentication token. For instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://bigml.io/source?username=merce_demo;api_key=********;limit=1\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl \"https://bigml.io/source?username=$BIGML_USERNAME;api_key=$BIGML_API_KEY;limit=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lists the last previously uploaded file in your account.<br/>\n",
    "Managing resources using these raw HTTP calls is, of course, possible but not optimal.<br/>\n",
    "Bindings to several languages will make easier resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bindings: connecting to BigML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, this notebook uses the **Python bindings library** and **BigMLer**, a command line utility, to access the BigML API. \n",
    "Please, check the **quick start** section of [BigMLer's documentation](http://bigmler.readthedocs.org/en/latest/#quick-start) to know how to **install** and remember to [set your **credentials**](http://bigml.readthedocs.org/en/latest/#authentication) before using **BigMLer** or the bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigml.api import BigML\n",
    "api = BigML() # Credentials are imported from environment variables\n",
    "              # BIGML_USERNAME and BIGML_API_KEY.\n",
    "              # They can also be set explicitly: api = BigML([username], [api_key])\n",
    "print(api.url + api.auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next instruction should show the last uploaded source in your account. If that's the case, your credentials are correctly set and you are ready to start creating your resources in BigML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api.list_sources(\"limit=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION WORKFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pprint import pprint, pformat\n",
    "from IPython.display import display\n",
    "# DIABETES_FILE = 'https://static.bigml.com/csv/diabetes.csv' # you can also import from a remote file\n",
    "DIABETES_FILE = 'data/diabetes.csv'\n",
    "display(pd.read_csv(DIABETES_FILE, nrows=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload this file and see how this content is interpreted from the Machine Learning point of view.<br/>\n",
    "We'll first create a **Project**, an organizational unit to store every resource generated in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"MLSEV Python bindings example\"\n",
    "project = api.create_project({'name': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projects** and **predictions** are the only resources that are **synchronous** in **BigML**, meaning that when you issue the create call the response you get is never a work in process, but the final resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first level attributes of this dictionary contain:\n",
    "\n",
    "- code: the HTTP response status code\n",
    "- error: the error information (when an HTTP error occurs)\n",
    "- location: the location to access the resource\n",
    "- object: the API's response\n",
    "- resource: the **resource ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT = project['resource']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of resources in **BigML** are **asynchronous**, so you will need polling for the resource till it is either finished or faulty. We'll see the first example now, when we upload our data to the platform and create a **source**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### CREATING SOURCE\n",
    "When data is uploaded to the platform a **source** is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = api.create_source(DIABETES_FILE,\n",
    "                           {'name': 'diabetes source', \\\n",
    "                            'tags': ['bindings example', 'diabetes'], \\\n",
    "                            'project': PROJECT})\n",
    "\"\"\"\n",
    "    CSV, ARFF, Excel and JSON files, either local or remote, can be uploaded.\n",
    "    For instance, you could use a remote diabetes:\n",
    "    DIABETES_FILE = \"https://static.bigml.com/csv/diabetes.csv\"\n",
    "    \n",
    "\"\"\"\n",
    "pprint(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this response does not contain any of the uploaded information yet. The **status** of the resource shows that the source creation request is in process. We'll have to wait for this process to finish. This is what **api.ok** does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api.ok(source)\n",
    "pprint(source[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**api.ok** waits for the source creation to finish and updates the contents of the **source** variable with the current remote version of the source. Thus, now we can see that the **source** variable contains the description of the fields inferred from the uploaded file. We'll write two auxiliar functions using **api.ok** to show the resources once they are finished or to warn us about any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(resource):\n",
    "    \"\"\"\n",
    "        Checks whether the resource status is *finished* or\n",
    "        prints an error if something fails.\n",
    "    \"\"\"\n",
    "    # api.ok uses api.get_[resouce_type] to retrieve the status of the resource\n",
    "    # till it reaches a final state (either FINISHED or FAULTY)\n",
    "    # as defined in \n",
    "    if not api.ok(resource):\n",
    "        print(\"Error!!!: Failed to create resource %s\" % \\\n",
    "            resource.get(\n",
    "                'resource',\n",
    "                resource.get('object', {}).get('name')))\n",
    "\n",
    "def check_and_show(resource):\n",
    "    \"\"\"\n",
    "        Checks whether the resource status is *finished*\n",
    "        and shows its contents or prints an error if something failed.\n",
    "    \"\"\"\n",
    "    check(resource)\n",
    "    pprint(resource)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View source in BIGML's web site\n",
    "As all **BigML**'s applications work on top of the same **API**, the source we've just created appears immediately in the source listings of our web dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIGML_DASHBOARD_URL = 'https://bigml.com/dashboard'\n",
    "sources_list_url = \"%s/sources\" % BIGML_DASHBOARD_URL\n",
    "print(sources_list_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is uploaded, we'd need to check that the **fields** characteristics inferred in our **source** are really the expected ones. As that's the case, the next step will be creating a **dataset** from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will provide information about **errors**, **missing** values in fields and **histograms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = api.create_dataset(source)\n",
    "check(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries show the number of **missings** and **errors** and we can decide what to do with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING MODEL\n",
    "This is the real training part, where the algorithm learns the patterns in your data. Un supervised learning models, the **objective field** is the label that will be predicted by the model. BigML considers the last field to be the objective field if not said otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = api.create_model( \\\n",
    "    dataset,\n",
    "    {'name': \"Diabetes decision tree\",\n",
    "     'objective_field': \"diabetes\"})\n",
    "check(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTIONS INTEGRATION\n",
    "Eventually, the goal of our models will usually be creating predictions. We can create **remote predictions** by providing the new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = {'plasma glucose': 180, 'bmi': 30}\n",
    "prediction = api.create_prediction(model,\n",
    "                                   input_data=input_data)\n",
    "check(prediction)\n",
    "print(\"prediction: %s\" % prediction[\"object\"][\"output\"])\n",
    "print(\"confidence: %s\" % prediction[\"object\"][\"confidence\"])\n",
    "print(\"path: %s\" % prediction[\"object\"][\"prediction_path\"][\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this method has latencies involved every time you make a prediction. If your predictions don't need to be immediate, then you can store the input data in a file and do a **remote batch prediction** with an entire test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class: using the model locally to predict\n",
    "The JSON model that can be downloaded via the API has all the information needed to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOCAL_MODEL_FILE = \"data/diabetes_model.json\"\n",
    "api.export(model, LOCAL_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The local **Model** object adds a **predict** method that can be used locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigml.model import Model\n",
    "\"\"\"\n",
    "    The **Model** object can use the contents of a Model\n",
    "    previously stored in a file or\n",
    "    internally download the model JSON structure once and\n",
    "    store it in a local directory for further use.\n",
    "\"\"\"\n",
    "local_model = Model(LOCAL_MODEL_FILE)\n",
    "pprint(local_model.predict(input_data, full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to predict many rows at once, you can use the **BigMLer** command line, that uses this local **Model** object to create the predictions and store it in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Basic prediction workflow\n",
    "\n",
    "To sum up, the basic prediction workflow will need some steps:\n",
    "\n",
    "- Upload the data to create a Source\n",
    "- Summarize all data in a Dataset\n",
    "- Create a Model from the Dataset\n",
    "- Use the Model to produce a prediction for the new data\n",
    "\n",
    "Using the diabetes example, to produce this workflow using the bindings you would use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from bigml.api import BigML\n",
    "from bigml.model import Model\n",
    "\n",
    "api = BigML()\n",
    "source = api.create_source(DIABETES_FILE, {\"project\": PROJECT})\n",
    "api.ok(source)\n",
    "dataset = api.create_dataset(source)\n",
    "api.ok(dataset)\n",
    "model = api.create_model(dataset)\n",
    "api.ok(model)\n",
    "\n",
    "local_model = Model(model)\n",
    "with open(\"data/diabetes_test.csv\") as test_handler:\n",
    "    reader = csv.DictReader(test_handler)\n",
    "    for input_data in reader:\n",
    "    # predicting for all rows\n",
    "        print(local_model.predict(input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same could be achieved in a single line command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIABETES = \"data/diabetes.csv\"\n",
    "DIABETES_TEST = \"data/diabetes_test.csv\"\n",
    "!bigmler --train $DIABETES --test $DIABETES_TEST \\\n",
    "         --output-dir diabetes-prediction \\\n",
    "         --project-id $PROJECT \\\n",
    "         --name \"Diabetes with BigMLer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to evaluate this model, we can add the **--evaluate** flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!bigmler --train $DIABETES \\\n",
    "         --test-split 0.2 \\\n",
    "         --output-dir diabetes-eval \\\n",
    "         --project-id $PROJECT \\\n",
    "         --name \"Diabetes evaluated with BigMLer\" \\\n",
    "         --seed \"bigml\" \\\n",
    "         --evaluate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
