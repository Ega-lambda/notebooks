{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The API: Connecting to BigML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The BigML API offers an endpoint to create, get, update and delete every Machine Learning resource. <br/>\n",
    "This API is accessible via HTTP and its general public domain is [bigml.io](https://bigml.io). <br/>\n",
    "You will need some credentials that will be used to authenticate every request. <br/>\n",
    "We recommend to set them in environment variables for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set your credentials as environment variables\n",
    "%env BIGML_USERNAME=[your username]\n",
    "%env BIGML_API_KEY=[your api key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API calls that you will need to issue contain these credentials as authentication token. For instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://bigml.io/source?username=[your username];api_key=[your api key];limit=1\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl \"https://bigml.io/source?username=$BIGML_USERNAME;api_key=$BIGML_API_KEY;limit=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lists the last previously uploaded file in your account.<br/>\n",
    "Managing resources using these raw HTTP calls is, of course, possible but not optimal.<br/>\n",
    "Bindings to several languages will make easier resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bindings: connecting to BigML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, this notebook uses the **Python bindings library** and **BigMLer**, a command line utility, to access the BigML API. \n",
    "Please, check the **quick start** section of [BigMLer's documentation](http://bigmler.readthedocs.org/en/latest/#quick-start) to know how to **install** and remember to [set your **credentials**](http://bigml.readthedocs.org/en/latest/#authentication) before using **BigMLer** or the bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigml.api import BigML\n",
    "api = BigML() # Credentials are imported from environment variables\n",
    "              # BIGML_USERNAME and BIGML_API_KEY.\n",
    "              # They can also be set explicitly: api = BigML([username], [api_key])\n",
    "print(api.url + api.auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data dictionary operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example uses a **Diabetes dataset**, which contains information about several features that might or might not be related to the users diagnose. The goal is predicting which features can influence the diagnose and predicting if a new user will have diabetes. The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pprint import pprint, pformat\n",
    "from IPython.display import display\n",
    "# DIABETES_FILE = 'https://static.bigml.com/csv/ext_diabetes.csv' # you can also import from a remote file\n",
    "DIABETES_FILE = 'data/ext_diabetes.csv'\n",
    "DIABETES_REMOTE = 'https://static.bigml.com/csv/ext_diabetes.csv'\n",
    "display(pd.read_csv(DIABETES_FILE, nrows=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload this file and see how this content is interpreted from the Machine Learning point of view.<br/>\n",
    "We'll first create a **Project**, an organizational unit to store every resource generated in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"VSSML18 Python bindings example\"\n",
    "project = api.create_project({'name': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projects** and **predictions** are the only resources that are **synchronous** in **BigML**, meaning that when you issue the create call the response you get is never a work in process, but the final resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first level attributes of this dictionary contain:\n",
    "\n",
    "- code: the HTTP response status code\n",
    "- error: the error information (when an HTTP error occurs)\n",
    "- location: the location to access the resource\n",
    "- object: the API's response\n",
    "- resource: the **resource ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT = project['resource']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of resources in **BigML** are **asynchronous**, so you will need polling for the resource till it is either finished or faulty. We'll see the first example now, when we upload our data to the platform and create a **source**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### CREATING SOURCE\n",
    "When data is uploaded to the platform a **source** is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = api.create_source(DIABETES_REMOTE,\n",
    "                           {'name': 'diabetes source', \\\n",
    "                            'tags': ['bindings example', 'diabetes'], \\\n",
    "                            'project': PROJECT})\n",
    "\"\"\"\n",
    "    CSV, ARFF, Excel and JSON files, either local or remote, can be uploaded.\n",
    "    For instance, you could use a remote diabetes:\n",
    "    DIABETES_FILE = \"https://static.bigml.com/csv/diabetes.csv\"\n",
    "    \n",
    "\"\"\"\n",
    "pprint(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this response does not contain any of the uploaded information yet. The **status** of the resource shows that the source creation request is in process. We'll have to wait for this process to finish. This is what **api.ok** does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api.ok(source)\n",
    "pprint(source[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**api.ok** waits for the source creation to finish and updates the contents of the **source** variable with the current remote version of the source. Thus, now we can see that the **source** variable contains the description of the fields inferred from the uploaded file. We'll write two auxiliar functions using **api.ok** to show the resources once they are finished or to warn us about any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(resource):\n",
    "    \"\"\"\n",
    "        Checks whether the resource status is *finished* or\n",
    "        prints an error if something fails.\n",
    "    \"\"\"\n",
    "    # api.ok uses api.get_[resouce_type] to retrieve the status of the resource\n",
    "    # till it reaches a final state (either FINISHED or FAULTY)\n",
    "    # as defined in \n",
    "    if not api.ok(resource):\n",
    "        print(\"Error!!!: Failed to create resource %s\" % \\\n",
    "            resource.get(\n",
    "                'resource',\n",
    "                resource.get('object', {}).get('name')))\n",
    "\n",
    "def check_and_show(resource):\n",
    "    \"\"\"\n",
    "        Checks whether the resource status is *finished*\n",
    "        and shows its contents or prints an error if something failed.\n",
    "    \"\"\"\n",
    "    check(resource)\n",
    "    pprint(resource)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View source in BIGML's web site\n",
    "As all **BigML**'s applications work on top of the same **API**, the source we've just created appears immediately in the source listings of our web dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIGML_DASHBOARD_URL = 'https://bigml.com/dashboard'\n",
    "sources_list_url = \"%s/sources\" % BIGML_DASHBOARD_URL\n",
    "print(sources_list_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is uploaded, we'd need to check that the **fields** characteristics inferred in our **source** are really the expected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fields class: working with fields\n",
    "What's the field structure that was inferred from the first lines of the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigml.fields import Fields\n",
    "fields = Fields(source) # retrieves the field structure from the source object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fields** attribute in **Fields** contains the complete fields structure information as a dictionary.<br/>\n",
    "It also has auxiliary functions to produce the field attributes, like the ID associated to each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(fields.field_id(\"medication\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATING SOURCE: changing fields type\n",
    "If you need to change any of the inferred types, just update your source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# medication was inferred to be a categorical field, it really is an items field\n",
    "# To update fields attributes, the expected format of the update body is\n",
    "# {\"fields\": {[field_id1]: {[field_attribute1]: [new_field_attribute1_value], \n",
    "#                          [field_attribute2]: [new_field_attribute2_value]},\n",
    "#             [field_id2]: {[field_attribute1]: [new_field_attribute1_value[,\n",
    "#                           [field_attribute2]: [new_field_attribute2_value]]}}\n",
    "\n",
    "fields_change  = {\n",
    "    fields.field_id('medication'): {'optype': 'items',\n",
    "                                    'item_analysis': {'separator': ';'}}}\n",
    "# updating the source structure\n",
    "source = api.update_source(source,\n",
    "                           {\"fields\": fields_change,\n",
    "                            \"name\": \"modified diabetes\"})\n",
    "check(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source has been updated and the field contents should be analyzed like a list of **items**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = Fields(source)\n",
    "fields.fields[fields.field_id(\"medication\")][\"optype\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a **categorical** field could be turned into a **text** field and their associated properties:\n",
    "```\n",
    "{\n",
    "    \"enabled\": true,\n",
    "    \"use_stopwords\": true,\n",
    "    \"stem_words\": true,\n",
    "    \"case_sensitive\": false,\n",
    "    \"language\": \"en\",\n",
    "    \"token_mode\": \"tokens_only\"\n",
    "}\n",
    "```\n",
    "could be changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missings can be a source of information in your data. For a correct management of missing values, we need to identify some strings that usually can be considered as such. These are considered **missing tokens**.<br/>\n",
    "In the example data, the field **bmi** contains a \"no data\" string which should be interpreted as a missing value.<br/>\n",
    "We can extend the list of missing values used by default in the **Source** adding this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# updating the source structure\n",
    "\n",
    "missing_tokens = fields.missing_tokens\n",
    "missing_tokens.append(\"no data\")\n",
    "source = api.update_source( \\\n",
    "    source,\n",
    "    {'source_parser': {\"missing_tokens\": missing_tokens,\n",
    "                       \"locale\": \"es-ES\"}})\n",
    "check(source)\n",
    "fields = Fields(source)\n",
    "\n",
    "pprint(fields.missing_tokens[0: 9])\n",
    "pprint(fields.missing_tokens[10: 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your **missings** have been correctly identified, it's time to analyze the full contents of the file and create a **Dataset** that summarizes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will provide information about **errors**, **missing** values in fields and **histograms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = api.create_dataset(source)\n",
    "check(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = Fields(dataset)\n",
    "print(\"The diabetes field contains: \", pformat(fields.fields[fields.field_id(\"diabetes\")][\"summary\"]))\n",
    "print(\"The bmi field has\", pformat(fields.fields[fields.field_id(\"bmi\")][\"summary\"][\"missing_count\"]) , \"missing values\")\n",
    "print(\"The pregnancies field has\", pformat(fields.fields[fields.field_id(\"pregnancies\")][\"summary\"][\"missing_count\"]) , \"missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries show the number of **missings** and **errors** and we can decide what to do with them.<br/>\n",
    "Some models can consider missing as a new value. In the field **pregnancies** we could associate it to the fact that the patient is a man.<br/>\n",
    "In this case, the model can be built to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = api.create_model(dataset, {\"missing_splits\": True})\n",
    "check(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other models, like **Clusters** cannot use the rows that have missing values. In this case, these rows are discarded when building the model unless the\n",
    "missing values are replaced with a sensible default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_args = {\"default_numeric_value\": \"mean\"}\n",
    "cluster = api.create_cluster(dataset, cluster_args)\n",
    "check(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing a new example, let's inspect this damaged **churn telemcom** dataset, where other datasets have been merged by mistake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHURN_FILE = 'https://static.bigml.com/csv/churn-telecom.csv' # you can also import from a remote file\n",
    "CHURN_FILE = 'data/churn-telecom.csv'\n",
    "CHURN_REMOTE = 'https://static.bigml.com/csv/churn-telecom.csv'\n",
    "display(pd.read_csv(CHURN_FILE, nrows=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should contain information about telecom accounts, where the last field **Churn** should be **True** if the user has churned and **False** otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirty_churn_source = api.create_source(CHURN_REMOTE, \\\n",
    "                                       {\"name\": \"Dirty churn\",\n",
    "                                        \"tags\": [\"bindings example\", \"dirty churn\"], \\\n",
    "                                        \"project\": project[\"resource\"]})\n",
    "api.ok(dirty_churn_source)\n",
    "dirty_churn_dataset = api.create_dataset(dirty_churn_source)\n",
    "api.ok(dirty_churn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the contents of the **Churn** field, we see that that's it has some unexpected values. We should remove the rows affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = Fields(dirty_churn_dataset)\n",
    "print(dirty_churn_dataset['object']['rows'])\n",
    "print(\"The Churn field contains: \", pformat(fields.fields[fields.field_id(\"Churn\")][\"summary\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_churn_dataset = api.create_dataset(dirty_churn_dataset, { \\\n",
    "    \"lisp_filter\": \"(or (= (f \\\"Churn\\\") \\\"True\\\") (= (f \\\"Churn\\\") \\\"False\\\"))\", \\\n",
    "    \"name\": \"Clean Churn\"})\n",
    "check(clean_churn_dataset)\n",
    "print(clean_churn_dataset['object']['rows'])\n",
    "fields = Fields(clean_churn_dataset)\n",
    "print(\"The Churn field contains: \", pformat(fields.fields[fields.field_id(\"Churn\")][\"summary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "New fields can help improve the performance of models. The **Churn** dataset has several fields that can be combined to generate new features, like ratios of charge per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [fields.field_name(col) for col in range(0, len(fields.fields))]\n",
    "prefix_fields = [name[0: -7] for name in names if name.endswith(\"charge\")]\n",
    "field_expressions = \" \".join([\"(/ (f \\\"%s charge\\\") (f \\\"%s calls\\\"))\" % ( \\\n",
    "    prefix_fields[index], prefix_fields[index]) \\\n",
    "    for index in range(0, len(prefix_fields))])\n",
    "fields_generator = [{\"names\": [\"%s charge per call\" % name for name in prefix_fields],\n",
    "                     \"fields\": \"(list %s)\" % field_expressions}]\n",
    "extended_dataset = api.create_dataset(clean_churn_dataset, {\"new_fields\": fields_generator})\n",
    "api.ok(extended_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the performance, we split the dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To ensure deterministic results, you must set the seed value\n",
    "SEED = \"BigML\"\n",
    "train_dataset = api.create_dataset(extended_dataset,\n",
    "                                   {'name': 'Churn train dataset (80%)',\n",
    "                                    'sample_rate': 0.8,\n",
    "                                    'seed': SEED})\n",
    "check(train_dataset)\n",
    "# The out_of_bag flag selects the instances left out in the previous dataset\n",
    "test_dataset = api.create_dataset(extended_dataset,\n",
    "                                  {'name': 'Churn test dataset (20%)',\n",
    "                                   'sample_rate': 0.8,\n",
    "                                   'out_of_bag': True,\n",
    "                                   'seed': SEED})\n",
    "check(test_dataset)\n",
    "print(\"train dataset: %s instances\" % train_dataset[\"object\"][\"rows\"])\n",
    "print(\"test dataset: %s instances\" % test_dataset[\"object\"][\"rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In our example, we will exclude the new fields first,\n",
    "excluded_fields = [\"%s charge per call\" % prefix for prefix in prefix_fields]\n",
    "\n",
    "# aternatively, you could write the list of fields to be included\n",
    "# usign \"input_fields\"\n",
    "original_model = api.create_model( \\\n",
    "    train_dataset,\n",
    "    {'name': \"Churn original fields\",\n",
    "     'objective_field': \"Churn\",\n",
    "     'excluded_fields': excluded_fields})\n",
    "\n",
    "check(original_model)\n",
    "used_fields = Fields(original_model[\"object\"][\"model\"][\"model_fields\"])\n",
    "used_fields.list_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_evaluation = api.create_evaluation(original_model, test_dataset, {\"name\": \"Churn original fields\"})\n",
    "api.ok(original_evaluation)\n",
    "print(original_evaluation['object']['result']['model']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will exclude now the original charge fields\n",
    "excluded_fields = [\"%s charge\" % prefix for prefix in prefix_fields]\n",
    "\n",
    "# aternatively, you could write the list of fields to be included\n",
    "# usign \"input_fields\"\n",
    "ratio_model = api.create_model( \\\n",
    "    train_dataset,\n",
    "    {'name': \"Churn ration fields\",\n",
    "     'objective_field': \"Churn\",\n",
    "     'excluded_fields': excluded_fields})\n",
    "\n",
    "check(ratio_model)\n",
    "used_fields = Fields(ratio_model[\"object\"][\"model\"][\"model_fields\"])\n",
    "used_fields.list_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_evaluation = api.create_evaluation(ratio_model, test_dataset, {\"name\": \"Churn ratio fields\"})\n",
    "api.ok(ratio_evaluation)\n",
    "print(ratio_evaluation['object']['result']['model']['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "Depending on your data, some configuration choices can produce better adapted models. As an example, using wights to balance the instances that end in churn can help the model to detect the patterns for this especially interesting class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "balanced_model = api.create_model( \\\n",
    "    train_dataset,\n",
    "    {'name': \"Churn ration fields\",\n",
    "     'objective_field': \"Churn\",\n",
    "     'balance_objective': True,\n",
    "     'excluded_fields': excluded_fields})\n",
    "balanced_evaluation = api.create_evaluation(balanced_model, test_dataset, {\"name\": \"Churn ratio fields\"})\n",
    "api.ok(balanced_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_metrics(evaluation, class_name):\n",
    "    \"\"\" Returns the evaluation metrics corresponding to a particular class\n",
    "    \n",
    "    \"\"\"\n",
    "    for class_info in evaluation['object']['result']['model']['per_class_statistics']:\n",
    "        if class_info[\"class_name\"] == class_name:\n",
    "            return class_info\n",
    "        \n",
    "print(\"Ratio model recall:\", get_metrics(ratio_evaluation, \"True\")[\"recall\"])\n",
    "print(\"Balanced model recall:\", get_metrics(balanced_evaluation, \"True\")[\"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTIONS INTEGRATION\n",
    "Eventually, the goal of our models will usually be creating predictions. Predictions can be created remotely by providing the new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = {'Total day minutes': 320, 'Number vmail messages': 2}\n",
    "prediction = api.create_prediction(ratio_model,\n",
    "                                   input_data=input_data)\n",
    "check(prediction)\n",
    "print(\"prediction: %s\" % prediction[\"object\"][\"output\"])\n",
    "print(\"confidence: %s\" % prediction[\"object\"][\"confidence\"])\n",
    "print(\"path: %s\" % prediction[\"object\"][\"prediction_path\"][\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this method has latencies involved every time you make a prediction. If your predictions don't need to be immediate, then you can store the input data in a file and do a batch prediction with an entire dataset of it. We can use our test dataset to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_prediction = api.create_batch_prediction(\\\n",
    "    ratio_model, test_dataset, \\\n",
    "    {\"all_fields\": True,\n",
    "     \"output_dataset\": True})\n",
    "check(batch_prediction)\n",
    "# we could download the results as a CSV using\n",
    "# api.download_batch_prediction(batch_prediction,\n",
    "#     filename='my_dir/my_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class: using the model locally to predict\n",
    "The JSON model that can be downloaded via the API has all the information needed to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOCAL_MODEL_FILE = \"data/churn_model.json\"\n",
    "api.export(ratio_model, LOCAL_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The local **Model** object adds a **predict** method that can be used locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigml.model import Model\n",
    "\"\"\"\n",
    "    The **Model** object can use the contents of a Model\n",
    "    previously stored in a file or\n",
    "    internally download the model JSON structure once and\n",
    "    store it in a local directory for further use.\n",
    "\"\"\"\n",
    "local_model = Model(LOCAL_MODEL_FILE)\n",
    "pprint(local_model.predict(input_data, full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to predict many rows at once, you can use the **BigMLer** command line, that uses this local **Model** object to create the predictions and store it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = ratio_model['resource']\n",
    "!bigmler --test $CHURN_REMOTE --model $MODEL_ID --output-dir predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you prefer the predictions to be computed remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHURN_TEST_REMOTE = 'https://static.bigml.com/csv/churn-test.csv'\n",
    "!bigmler --test $CHURN_TEST_REMOTE --model $MODEL_ID --output-dir remote-predictions --remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Basic prediction workflow\n",
    "\n",
    "To sum up, the basic prediction workflow will need some steps:\n",
    "\n",
    "- Upload the data to create a Source\n",
    "- Summarize all data in a Dataset\n",
    "- Create a Model from the Dataset\n",
    "- Use the Model to produce a prediction for the new data\n",
    "\n",
    "Using the diabetes example, to produce this workflow using the bindings you would use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from bigml.api import BigML\n",
    "from bigml.model import Model\n",
    "\n",
    "api = BigML()\n",
    "source = api.create_source(DIABETES_REMOTE, {\"project\": PROJECT})\n",
    "api.ok(source)\n",
    "dataset = api.create_dataset(source)\n",
    "api.ok(dataset)\n",
    "model = api.create_model(dataset)\n",
    "api.ok(model)\n",
    "\n",
    "local_model = Model(model)\n",
    "with open(\"data/diabetes_test.csv\") as test_handler:\n",
    "    reader = csv.DictReader(test_handler)\n",
    "    for input_data in reader:\n",
    "    # predicting for all rows\n",
    "        print(local_model.predict(input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same could be achieved in a single line command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIABETES_TEST_REMOTE = \"https://static.bigml.com/csv/diabetes_test.csv\"\n",
    "!bigmler --train $DIABETES_REMOTE --test $DIABETES_TEST_REMOTE \\\n",
    "         --output-dir diabetes-prediction --project-id $PROJECT \\\n",
    "         --name \"Diabetes with bigmler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to evaluate this model, we can add the **--evaluate** flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!bigmler --train $DIABETES_REMOTE --output-dir diabetes-eval --evaluate \\\n",
    "         --project-id $PROJECT --name \"Diabetes evaluated with BigMLer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers removal workflow \n",
    "We can try to improve that performance by removing the top outliers from the dataset before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_ID = dataset['resource']\n",
    "!bigmler anomaly --dataset $DATASET_ID \\\n",
    "                 --anomaly-fields \"insulin,pregnancies,plasma glucose,diabetes\" \\\n",
    "                 --top-n 2 --anomalies-dataset out --output-dir diabetes_anomaly \\\n",
    "                 --project-id $PROJECT --name \"Clean diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluating the model built on the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!bigmler --datasets diabetes_anomaly/dataset_gen --output-dir diabetes-clean-eval \\\n",
    "         --project-id $PROJECT --evaluate --name \"Clean diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain with cumulative data\n",
    "\n",
    "Usually, you start your project uploading a sample of data and playing with it till you discover the workflow that gives you acceptable results. Then, the rest of data is uploaded and you'd like to repeat the same process on the accumulated data. **BigMLer** can help you do that. In this example, we do a regular model creation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DIABETES_REMOTE = \"https://static.bigml.com/csv/ext_diabetes_1.csv\" \n",
    "# download this file and save it as a local file in data/ext_diabetes_1.csv\n",
    "DIABETES_1 = \"data/ext_diabetes_1.csv\"\n",
    "!bigmler --train $DIABETES_1 \\\n",
    "         --tag cumulative_diabetes \\\n",
    "         --name \"Cumulative diabetes data\" \\\n",
    "         --project-id $PROJECT \\\n",
    "         --output-dir ./initial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after that, new data is uploaded and the same process is reproduced on the accumulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DIABETES_REMOTE_2 = \"https://static.bigml.com/csv/ext_diabetes_2.csv\"\n",
    "# download this file and store it in data/ext_diabetes_2.csv\n",
    "DIABETES_2 = \"data/ext_diabetes_2.csv\"\n",
    "!bigmler retrain --add $DIABETES_2 \\\n",
    "                 --model-tag cumulative_diabetes \\\n",
    "                 --output-dir accumulative_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
